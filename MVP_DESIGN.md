# Startup Role Simulator - MVP Backend & Simulation Agent Design Document

## Purpose

This document defines the **technical design and implementation specifications** for the MVP backend and simulation agent of the Startup Role Simulator. It serves as the primary reference for LLM-assisted development and ensures consistency across development sessions.

**Context for LLMs**: This is a reflective educational tool (not a game). Users experience scenarios from different startup roles, make constrained choices, and receive insight through LLM-powered role voice conversations. The backend orchestrates scenarios, tracks decisions, generates role profiles, and manages reflective conversations.

---

## 1. Project Architecture Overview

### System Components

```
Frontend (Separate Repo)
    ↓
Backend API (This Repo)
    ├── API Layer (FastAPI)
    ├── Session Management
    ├── Scenario Engine
    ├── Decision Tracker & Pattern Analyzer
    ├── Role Profile Generator
    └── LLM Integration Layer
        ├── Role Voice Conversation System
        └── LLM Client (OpenAI/Gemini)
```

### Technology Stack

- **Backend Framework**: FastAPI (Python 3.10+)
- **LLM Provider**: OpenAI (GPT-4 or GPT-3.5-turbo) or Google Gemini
- **Session Storage**: In-memory dictionary (MVP) - can be replaced with SQLite/Redis later
- **Data Validation**: Pydantic models
- **HTTP Client**: httpx or openai/Google client libraries

---

## 2. Core Data Models

### 2.1 Role Enumeration

```python
# Pseudocode representation
Role = Enum {
    "engineer": "Early-stage Engineer",
    "product_manager": "Product Manager",
    "founder": "Founder / CEO"
}
```

Each role has a unique identifier and display name.

### 2.2 Scenario Structure

A scenario is a static data structure (hand-authored JSON/YAML or Python dict):

```python
Scenario = {
    "id": str,  # unique identifier (e.g., "engineer_scenario_1")
    "role": Role,  # which role this scenario belongs to
    "title": str,  # short scenario title
    "description": str,  # scenario context (2-3 paragraphs)
    "choices": [
        {
            "id": str,  # unique choice id
            "text": str,  # choice description
            "traits": List[str]  # associated decision traits (e.g., ["long_term", "quality_focused"])
        },
        # 3-4 total choices
    ],
    "order": int  # display order for this role (1-7)
}
```

**Important**: Scenarios are deterministic and hand-authored. They are NOT generated by LLM.

### 2.3 Session State

```python
Session = {
    "session_id": UUID,
    "role": Role,
    "created_at": datetime,
    "current_phase": str,  # "scenarios" | "profile_generation" | "reflection"
    "scenarios_completed": List[ScenarioResponse],
    "role_profile": Optional[RoleProfile],
    "reflection_conversation": Optional[ConversationHistory]
}
```

### 2.4 Scenario Response (User Decision)

```python
ScenarioResponse = {
    "scenario_id": str,
    "choice_id": str,
    "timestamp": datetime
}
```

### 2.5 Decision Pattern (Internal Analysis)

The system analyzes all scenario responses to identify patterns:

```python
DecisionPattern = {
    "trait_frequency": Dict[str, int],  # count of trait appearances
    "common_trade_offs": List[str],  # e.g., ["long_term vs short_term", "quality vs speed"]
    "dominant_preferences": List[str],  # top 2-3 patterns
    "tension_areas": List[str]  # areas where decisions conflicted
}
```

### 2.6 Role Profile (Output)

```python
RoleProfile = {
    "summary": str,  # 2-3 sentence overview
    "decision_patterns": List[str],  # 3-5 pattern descriptions
    "trade_off_preferences": List[str],  # how user balances competing priorities
    "key_insights": List[str]  # 2-3 reflective insights
}
```

**Important**: The role profile is generated deterministically from decision patterns. It uses simple rule-based logic or templates (NOT LLM initially, but can be enhanced later).

### 2.7 Role Voice Conversation

```python
ConversationMessage = {
    "role": "user" | "assistant",
    "content": str,
    "timestamp": datetime
}

ConversationHistory = List[ConversationMessage]
```

---

## 3. API Endpoints Specification

### 3.1 POST `/api/sessions/create`

**Purpose**: Initialize a new simulation session.

**Request Body**:

```json
{
    "role": "engineer" | "product_manager" | "founder"
}
```

**Response**:

```json
{
    "session_id": "uuid-string",
    "role": "engineer",
    "first_scenario": {
        "id": "engineer_scenario_1",
        "title": "...",
        "description": "...",
        "choices": [...]
    },
    "total_scenarios": 5
}
```

**Behavior**:

- Generate UUID for session
- Store session in memory
- Return first scenario for selected role
- Initialize empty scenarios_completed list

### 3.2 GET `/api/sessions/{session_id}/scenario/{scenario_id}`

**Purpose**: Retrieve a specific scenario (if needed for navigation/refresh).

**Response**:

```json
{
    "id": "engineer_scenario_1",
    "title": "...",
    "description": "...",
    "choices": [...]
}
```

### 3.3 POST `/api/sessions/{session_id}/decide`

**Purpose**: Submit user's choice for current scenario and get next scenario.

**Request Body**:

```json
{
  "scenario_id": "engineer_scenario_1",
  "choice_id": "choice_1"
}
```

**Response**:

```json
{
    "next_scenario": {
        "id": "engineer_scenario_2",
        "title": "...",
        "description": "...",
        "choices": [...]
    } | null,  # null if all scenarios completed
    "scenarios_completed": 2,
    "total_scenarios": 5,
    "is_complete": false
}
```

**Behavior**:

- Validate scenario_id and choice_id
- Store ScenarioResponse in session
- If more scenarios exist, return next scenario
- If all scenarios complete, return null for next_scenario and set is_complete=true

### 3.4 POST `/api/sessions/{session_id}/generate-profile`

**Purpose**: Generate role profile after all scenarios are completed.

**Request**: No body required.

**Response**:

```json
{
    "role_profile": {
        "summary": "...",
        "decision_patterns": [...],
        "trade_off_preferences": [...],
        "key_insights": [...]
    }
}
```

**Behavior**:

- Verify all scenarios are completed
- Analyze decision patterns from all scenario responses
- Generate role profile (rule-based or template-based)
- Store profile in session
- Update session phase to "profile_generation"

### 3.5 POST `/api/sessions/{session_id}/reflection/start`

**Purpose**: Initialize role voice conversation for reflection.

**Request**: No body required.

**Response**:

```json
{
  "role_voice_message": "Hello! I'm here to reflect with you about your decisions...",
  "conversation_id": "conv-uuid"
}
```

**Behavior**:

- Verify profile exists
- Initialize conversation history
- Generate initial role voice message using LLM
- Store conversation in session
- Update session phase to "reflection"

### 3.6 POST `/api/sessions/{session_id}/reflection/message`

**Purpose**: Continue role voice conversation.

**Request Body**:

```json
{
  "message": "user's message text"
}
```

**Response**:

```json
{
  "role_voice_message": "response from role voice",
  "conversation_complete": false,
  "turn_count": 3
}
```

**Behavior**:

- Validate conversation exists
- Append user message to conversation history
- Call LLM with role voice prompt + conversation history + role profile
- Append assistant response to conversation history
- Check turn count (max 6 turns)
- Return response and completion status

### 3.7 GET `/api/sessions/{session_id}/status`

**Purpose**: Get current session state (for frontend state management).

**Response**:

```json
{
    "session_id": "uuid",
    "role": "engineer",
    "phase": "scenarios" | "profile_generation" | "reflection",
    "scenarios_completed": 2,
    "total_scenarios": 5,
    "has_profile": false,
    "has_reflection": false
}
```

### 3.8 Error Responses

All endpoints should return consistent error format:

```json
{
    "error": "error_type",
    "message": "human-readable message",
    "details": {}  # optional additional context
}
```

**Error Types**:

- `INVALID_SESSION`: Session ID not found
- `INVALID_ROLE`: Role not recognized
- `INVALID_SCENARIO`: Scenario ID not found or doesn't belong to role
- `INVALID_CHOICE`: Choice ID not found or doesn't belong to scenario
- `INVALID_PHASE`: Operation not allowed in current phase
- `LLM_ERROR`: LLM API call failed
- `INTERNAL_ERROR`: Unexpected server error

---

## 4. Session Management

### 4.1 Storage Strategy (MVP)

Use in-memory dictionary:

```python
sessions: Dict[UUID, Session] = {}
```

**Limitations**:

- Sessions lost on server restart
- No persistence across instances
- Suitable for MVP/demo only

**Future Enhancement**: Replace with SQLite, Redis, or database.

### 4.2 Session Lifecycle

1. **Creation**: `/api/sessions/create` creates new session
2. **Active**: Session used during scenario completion
3. **Profile Generation**: After all scenarios, profile generated
4. **Reflection**: Conversation phase (5-6 turns)
5. **Complete**: Session archived (can be kept for 24 hours, then purged)

### 4.3 Session Validation

Every endpoint should:

- Check session_id exists
- Validate phase allows requested operation
- Return appropriate errors if invalid

---

## 5. Scenario Engine

### 5.1 Scenario Storage

Scenarios can be stored as:

- Python dictionaries in a `scenarios.py` file
- JSON files in `data/scenarios/` directory
- YAML files (if preferred)

**Structure**:

```
data/
  scenarios/
    engineer_scenarios.json
    product_manager_scenarios.json
    founder_scenarios.json
```

### 5.2 Scenario Loading

On server startup:

- Load all scenarios into memory
- Validate scenario structure
- Index by role and scenario_id for fast lookup

### 5.3 Scenario Progression

- Scenarios are presented in `order` field sequence
- After each decision, move to next scenario
- Track completion in session state

---

## 6. Decision Pattern Analysis

### 6.1 Pattern Detection Logic

After all scenarios completed, analyze all ScenarioResponses:

1. **Extract Traits**: For each choice, get associated traits
2. **Count Frequencies**: Count how many times each trait appears
3. **Identify Dominant Patterns**: Top 2-3 most common trait categories
4. **Detect Tensions**: Find scenarios where conflicting traits were chosen
5. **Summarize Trade-offs**: Group related trade-off pairs

### 6.2 Trait Categories (Examples)

For Engineer role:

- `long_term` vs `short_term`
- `quality_focused` vs `speed_focused`
- `user_facing` vs `technical_debt`
- `collaborative` vs `independent`
- `risk_averse` vs `risk_taking`

For Product Manager:

- `data_driven` vs `intuition`
- `user_centric` vs `business_centric`
- `rapid_iteration` vs `strategic_planning`
- `consensus_building` vs `decisive`

For Founder:

- `vision_focused` vs `execution_focused`
- `growth_at_all_costs` vs `sustainable_growth`
- `hands_on` vs `delegation`
- `customer_focus` vs `investor_focus`

### 6.3 Pattern Analysis Output

Convert decision patterns into human-readable descriptions:

Example:

- Input: `trait_frequency = {"long_term": 4, "quality_focused": 5, "risk_averse": 3}`
- Output: `"You consistently prioritize long-term quality, even when short-term delivery is at risk."`

---

## 7. Role Profile Generation

### 7.1 Generation Method (MVP)

Use rule-based template system:

1. Input: DecisionPattern analysis
2. Process: Match patterns to template rules
3. Output: Structured RoleProfile

**Template Structure**:

- Summary: Generic intro + dominant pattern mention
- Decision Patterns: 3-5 sentences describing key patterns
- Trade-off Preferences: How user balanced competing priorities
- Key Insights: 2-3 reflective observations

**Future Enhancement**: Can be enhanced with LLM generation (structured output) for more nuanced profiles.

### 7.2 Profile Format

Profiles should be:

- Insightful but not prescriptive
- Descriptive, not evaluative
- Focused on patterns, not scores
- Respectful and non-judgmental

---

## 8. LLM Integration - Role Voice Conversations

### 8.1 LLM Provider Selection

**Primary**: OpenAI (GPT-4 or GPT-3.5-turbo)
**Alternative**: Google Gemini

### 8.2 LLM Configuration

- **Temperature**: 0.0 - 0.2 (deterministic)
- **Max Tokens**: 200-300 per response
- **Model**: GPT-4-turbo-preview or GPT-3.5-turbo (cost/quality trade-off)

### 8.3 Role Voice Prompt Structure

```
System Prompt (Role Voice Persona):

You are the voice of a [ROLE] archetype in a startup simulation tool.
Your purpose is to help users reflect on their decisions, not to judge or advise.

Role Context:
- [Role-specific worldview and priorities]
- [How this role typically thinks]

Your Conversation Style:
- Speak from the role's perspective
- Validate or gently challenge the user's thinking
- Highlight patterns you notice
- Avoid giving advice or prescriptions
- Keep responses concise (2-3 sentences max)
- Never claim to be a real person

Current Context:
- User completed scenarios with these patterns: [DecisionPattern summary]
- Role Profile: [RoleProfile summary]

Conversation History:
[Previous messages]

User Message: [Current user message]

Respond as the role voice:
```

### 8.4 Conversation Management

- **Turn Limit**: Maximum 6 turns (3 user messages, 3 role voice responses after initial message)
- **Conversation History**: Include last 4-6 messages in context (avoid token bloat)
- **Termination**: After turn limit, mark conversation as complete

### 8.5 Response Caching (Optional MVP Enhancement)

To ensure determinism:

- Cache LLM responses by: (role, conversation_state_hash, user_message_hash)
- If cache hit, return cached response
- Cache key should include relevant context (profile summary, conversation history)

---

## 9. Error Handling

### 9.1 LLM Error Handling

If LLM API call fails:

- Return `LLM_ERROR` status
- Include fallback message: "I'm having trouble responding right now. Let's try again."
- Log error for debugging
- Don't crash the session

### 9.2 Validation Errors

- Validate all inputs using Pydantic models
- Return descriptive error messages
- Don't expose internal errors to frontend

### 9.3 Session Errors

- If session not found, return clear error
- If session in wrong phase, explain what operation is expected

---

## 10. MVP Scope Boundaries

### Included in MVP:

✅ 3 roles (Engineer, PM, Founder)
✅ 5-7 scenarios per role (hand-authored)
✅ Constrained choice selection (3-4 choices per scenario)
✅ Decision pattern analysis (rule-based)
✅ Role profile generation (template-based)
✅ LLM-powered role voice conversations (5-6 turns)
✅ Session management (in-memory)
✅ Basic error handling

### Explicitly Excluded from MVP:

❌ Multi-agent interactions
❌ Score/ranking system
❌ Scenario generation by LLM
❌ Database persistence
❌ User authentication
❌ Analytics/metrics collection
❌ Role comparison mode
❌ Team scenarios
❌ Real-time collaboration
❌ Mobile app
❌ Caching layer (optional enhancement)

---

## 11. Implementation Phases

### Phase 1: Core Infrastructure

- Set up FastAPI project structure ✅
- Define Pydantic models
- Implement session management (in-memory)
- Create basic API endpoints structure

### Phase 2: Scenario System

- Create scenario data structures
- Implement scenario loading and retrieval
- Build scenario progression logic
- Add scenario validation

### Phase 3: Decision Tracking

- Implement decision storage
- Build pattern analysis logic
- Create trait extraction system

### Phase 4: Profile Generation

- Design template system
- Implement profile generation logic
- Test pattern-to-profile conversion

### Phase 5: LLM Integration

- Set up LLM client
- Create role voice prompt templates
- Implement conversation management
- Add error handling for LLM calls

### Phase 6: API Completion

- Complete all endpoint implementations
- Add comprehensive error handling
- Write request/response validation
- Test end-to-end flows

### Phase 7: Polish & Testing

- Add logging
- Write basic integration tests
- Document API (OpenAPI/Swagger)
- Performance optimization if needed

---

## 12. Key Design Principles

1. **Simplicity Over Complexity**: MVP should be finishable, not perfect
2. **Determinism**: Same inputs produce same outputs (especially for LLM)
3. **Clear Boundaries**: LLM for reflection only, not core logic
4. **Human-Centered**: Focus on insight and empathy, not evaluation
5. **No Scores**: Avoid numeric evaluations or rankings
6. **Finishable**: Prefer working simple solution over incomplete complex one

---

## 13. Success Criteria for MVP

The backend is complete when:

- ✅ All API endpoints function correctly
- ✅ A user can complete full simulation flow (scenarios → profile → reflection)
- ✅ LLM conversations are consistent and role-appropriate
- ✅ Error handling covers all edge cases
- ✅ Code is clean, documented, and maintainable
- ✅ API can be consumed by separate frontend repository

---

## 14. Notes for LLM Assistants

When working on this project:

1. **Always refer to this document** for context
2. **Maintain MVP scope** - don't add features not listed
3. **Follow the data models** - use Pydantic for validation
4. **Keep LLM usage focused** - only for role voice conversations
5. **Prioritize finishing** - working simple code > incomplete complex code
6. **Test incrementally** - verify each phase before moving forward
7. **Document decisions** - add comments for non-obvious logic

---

## 15. Example Scenario Structure

Here's an example of how a scenario should be structured:

```json
{
  "id": "engineer_scenario_1",
  "role": "engineer",
  "title": "Technical Debt vs Feature Request",
  "description": "The PM has prioritized a new feature that users are requesting. However, you've identified critical technical debt that's causing 20% of user bugs. The refactor would take 2 weeks, same as the feature. The PM is pushing for the feature to ship this sprint.",
  "choices": [
    {
      "id": "engineer_1_choice_1",
      "text": "Advocate strongly for the refactor first, explaining the long-term impact of technical debt",
      "traits": ["long_term", "quality_focused", "risk_averse"]
    },
    {
      "id": "engineer_1_choice_2",
      "text": "Ship the feature but document the technical debt as a blocker for next sprint",
      "traits": ["short_term", "collaborative", "pragmatic"]
    },
    {
      "id": "engineer_1_choice_3",
      "text": "Work extra hours to do both this sprint",
      "traits": ["hands_on", "independent", "quality_focused"]
    }
  ],
  "order": 1
}
```

---

## 16. Environment Variables

Required environment variables:

```bash
LLM_PROVIDER=openai  # or "gemini"
OPENAI_API_KEY=sk-...  # if using OpenAI
GEMINI_API_KEY=...  # if using Gemini
OPENAI_MODEL=gpt-4-turbo-preview  # or gpt-3.5-turbo
LOG_LEVEL=INFO
```

---

## 17. API Documentation

FastAPI will automatically generate OpenAPI/Swagger documentation at `/docs` endpoint.

Ensure all endpoints have:

- Clear docstrings
- Request/response models documented
- Example values where helpful

---

This document should serve as the single source of truth for MVP backend implementation. Refer to it when making implementation decisions or when context is needed across different development sessions.
